---
title: "Replication of \"Predicting Users' First Impressions of Website Aesthetics With a Quantification of Perceived Visual Complexity and Colorfulness\" by Reinecke et al. (2013, CHI)"
author: "Mackenzie Leake (mleake@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

For this project I chose the paper "Predicting Users' First Impressions of Website Aesthetics With a Quantification of Perceived Visual Complexity and Colorfulness" by Reinecke et al. This paper was published at CHI 2013, which is a publication venue that is very relevant to my research in HCI and computer graphics. I chose this paper on evaluating users' first impressions of websites because I am interested in learning several aspects of their methodology. I have never used MTurk to run experiments, though it is common in my research area, and this is something I would like to learn in this project. I am also interested in how the authors took something subjective like the visual appeal of websites and created more objective criteria to explore specific questions. This is relevant to my work as a CS PhD student, which focuses on building video editing systems. I am often tasked with developing computational models of creative decisions and evaluating users' perceptions and experiences using the tools I build.

I am interested in replicating the initial experiments discussed in the section "Evaluation of Image Metrics" on pages 4-7 in the paper. Essentially, the researchers evaluate if they can use computational metrics of color and visual complexity to predict user ratings of these website qualities. To run these experiments they collected a set of 450 website image screenshots. These included 350 in English, 60 foreign websites, 20 grayscale images, and 20 websites that were nominated for Webby Awards. For a follow up paper the authors provided a zip file with all of these website images, and I am able to download and open these images. I would hope to use these same images in my replication project. The researchers showed each participant 30 website images in random order for 500ms and asked each participant to rate each website on a 9-point Likert scale from "not at all colorful" to "very colorful" and "not at all complex" to "very complex." They also collected demographic data, including age, gender, education level, and country of residence, though their analysis indicates that none of these demographic variables affected their model of colorfulness and visual complexity. 

To replicate their experiment I plan to start with the same website images they provide and use their code for computing image metrics. Though the authors provide their (not particularly well-documented) Java code for computing visual features for website images and describe the different metrics, I expect there to be some challenge in running this code and understanding the different metrics. I will then need to set up an experiment on MTurk to show the provided website images to people for the same time period (500ms) and collect their ratings for colorfulness and complexity using the same Likert scale as the original experiments. Since I have not run a MTurk experiment before, I think that setting up the experiment and showing images for a fixed amount of time to participants will require some learning on my part. Once I compute the image metrics and collect data on MTurk, I will need to compute regression models to compare the predicted colorfulness and visual complexity data with the observations from the MTurk data. The authors discuss using multiple linear regression with backward elimination, which is not a statistical technique I have used before. Though replicating this project has several challenges in terms of figuring out the original authors' computational image metrics, running a MTurk experiment, and using unfamiliar statistical techniques, I think that each of these challenges is relevant to other challenges I will face in my own research. 

Link to repo: [link](https://github.com/mleake/reinecke2013)  
Link to paper: [link](https://github.com/mleake/reinecke2013/blob/master/original_paper/reinecke2013.pdf)  
(note: My Github account is part of the Stanford HCI organization so I can't send links to private repos to non-members. I will upload the .zip repo and/or add the TAs as collaborators.)

##Methods

###Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

###Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

###Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Procedure	

Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

###Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


##Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
###Data Preparation

####Load Relevant Libraries and Functions

####Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
