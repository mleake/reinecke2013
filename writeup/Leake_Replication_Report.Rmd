---
title: "Replication of \"Predicting Users' First Impressions of Website Aesthetics With a Quantification of Perceived Visual Complexity and Colorfulness\" by Reinecke et al. (2013, CHI)"
author: "Mackenzie Leake (mleake@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Project proposal from 10/5
For this project I chose the paper "Predicting Users' First Impressions of Website Aesthetics With a Quantification of Perceived Visual Complexity and Colorfulness" by Reinecke et al. This paper was published at CHI 2013, which is a publication venue that is very relevant to my research in HCI and computer graphics. I chose this paper on evaluating users' first impressions of websites because I am interested in learning several aspects of their methodology. I have never used MTurk to run experiments, though it is common in my research area, and this is something I would like to learn in this project. I am also interested in how the authors took something subjective like the visual appeal of websites and created more objective criteria to explore specific questions. This is relevant to my work as a CS PhD student, which focuses on building video editing systems. I am often tasked with developing computational models of creative decisions and evaluating users' perceptions and experiences using the tools I build.

I am interested in replicating the initial experiments discussed in the section "Evaluation of Image Metrics" on pages 4-7 in the paper. Essentially, the researchers evaluate if they can use computational metrics of color and visual complexity to predict user ratings of these website qualities. To run these experiments they collected a set of 450 website image screenshots. These included 350 in English, 60 foreign websites, 20 grayscale images, and 20 websites that were nominated for Webby Awards. For a follow up paper the authors provided a zip file with all of these website images, and I am able to download and open these images. I would hope to use these same images in my replication project. The researchers showed each participant 30 website images in random order for 500ms and asked each participant to rate each website on a 9-point Likert scale from "not at all colorful" to "very colorful" and "not at all complex" to "very complex." They also collected demographic data, including age, gender, education level, and country of residence, though their analysis indicates that none of these demographic variables affected their model of colorfulness and visual complexity. 

To replicate their experiment I plan to start with the same website images they provide and use their code for computing image metrics. Though the authors provide their (not particularly well-documented) Java code for computing visual features for website images and describe the different metrics, I expect there to be some challenge in running this code and understanding the different metrics. I will then need to set up an experiment on MTurk to show the provided website images to people for the same time period (500ms) and collect their ratings for colorfulness and complexity using the same Likert scale as the original experiments. Since I have not run a MTurk experiment before, I think that setting up the experiment and showing images for a fixed amount of time to participants will require some learning on my part. Once I compute the image metrics and collect data on MTurk, I will need to compute regression models to compare the predicted colorfulness and visual complexity data with the observations from the MTurk data. The authors discuss using multiple linear regression with backward elimination, which is not a statistical technique I have used before. Though replicating this project has several challenges in terms of figuring out the original authors' computational image metrics, running a MTurk experiment, and using unfamiliar statistical techniques, I think that each of these challenges is relevant to other challenges I will face in my own research. 

Link to repo: [link](https://github.com/mleake/reinecke2013)  
Link to paper: [link](https://github.com/mleake/reinecke2013/blob/master/original_paper/reinecke2013.pdf)

##Introduction
In their original paper "Predicting Users' First Impressions of Website Aesthetics With a Quantification of Perceived Visual Complexity and Colorfulness" Reinecke et al. propose a computational model for predicting users' initial reactions to a website's appearance. This paper builds upon the existing literature that indicates that colorfulness and visual complexity are the two primary design features that contribute to website aesthetics. The authors propose models for predicting users' ratings of colorfulness and visual complexity based on automatically computed low-level image features. The focus of this replication is the colorfulness model.

In this replication of the colorfulness experiment conducted by Reinecke et al. (2013), we aim to reproduce the methodology used in the original paper. First, we use the authors' provided low-level image features of the website images. These features include the basic HTML colors as well as measures of colorfulness and information density. Next, we conduct an experiment on MTurk, asking users to rate the colorfulness of these images. From the data we develop a computational model relating the low-level image statistics to users' ratings of colorfulness, as the original authors did.

##Methods

###Power Analysis
For this particular analysis, power analysis is not appropriate. We use 100 participants and have each rate 30 images so that we have enough ratings for each of the 450 randomly selected website images.

###Planned Sample
We use a similar sample to the participant sample selected in the original paper, described as:

> "Our final data consist of 184 participants (96 female) aged between 15 and 58 years (mean = 21.1) for the colorfulness experiment and 122 participants (60 female) between 16 and 70 years (mean = 32.3) for the complexity experiment." In our study we start with 100 participants.

We also follow the exclusion criteria specified in the original paper:

> "Participants were excluded from the analysis if they had previously participated in the same study and/or did not have normal or corrected-to-normal vision."

###Materials
We use the same materials as the original authors. This includes the 450 website images provided by the authors as well as the computed visual features, both of which are available [here](http://www.labinthewild.org/data/index.php).

###Procedure	
We follow a similar procedure to the one described in the original article as:

> "The experiments were implemented as 10-minute online tests on our own experimental platform LabintheWild.org and advertised in online communities and university newsletters. Both experiments followed the same pattern: Participants were asked to rate screenshots of websites that were shown for 500ms each (following the experiment procedure in [23]). The small exposure time avoids an in-depth engagement with the content of the sites, and instead captures participants’ initial reactions towards colorfulness and visual complexity. In the first evaluation phase, we presented each participant with a stratified random sample of 30 websites selected from the larger pool of 450 websites. The 22 English, 4 foreign (using a different script), and 4 grayscale websites were presented in random order. Participants rated every website on a 9-point Likert scale from “not at all complex” to “very complex” or “not at all colorful” to “very colorful”, depending on the experiment. After being encouraged to take a short break, we gave a second evaluation phase where participants re-rated the same 30 websites in a different random order so that we could measure consistency in their judgement. Before the two evaluation phases, we gave a short practice phase during which all participants were asked to evaluate a fixed set of five websites, given in randomized order. We also collected demographic information about each participant, such as gender, age, education level, and current country of residence, in order to control for these factors in the analysis."

###Analysis Plan
The goal of this analysis is to replicate the first set of experiments in Reinecke et. al, 2013. For all analyses we follow the the first of the data exclusion criteria the original authors describe: 

> "We additionally excluded websites that had received three or fewer ratings (due to the random assignment to participants), or where the standard error of their mean complexity was ≥ 0.75." 

We do not follow the second guideline for data exclusion because this replication does not collect data on mean complexity.

The primary analysis to perform is computing the prediction model for perceived colorfulness. For this analysis we use multiple linear regression with backward elimination. The model begins the authors' original set of image statistics, and then different variables are removed according to the process described in the original paper: 

>"In this method, all predictors are initially added to the model, and iteratively removed if they do not make a statistically significant contribution to how well the model predicts the outcome variable (the ratings, in our case). At each step, the remaining predictors are reassessed in terms of their contribution to the newly calculated model." 

The goal of these analyses is to replicate the paper's models for predicting users' ratings of colorfulness based on specific low-level image features generated computationally.

###Differences from Original Study
All experiments were run on MTurk as opposed to Lab in the Wild, which has an unpaid citizen science reward model. Characterizing participants on Lab in the Wild is an area of ongoing research. So far researchers have found that participants come from almost every country in the world and users are motivated to participate because they get to see how their results compare to others (Reinecke & Gajos, 2015). We do not anticipate that the difference in experiment platform will influence the results in this study. The original study participants were as young as 15, but in this replication the minimum age is set at 18, as required by MTurk. One step in the authors' original experiments is determining if users' demographic backgrounds influence their judgment of website colorfulness and visual complexity. The original finding is that these demographic variables do not influence these specific aesthetic judgments.



```{r echo = TRUE}
#note later switch to {r include = F}
####Load Relevant Libraries and Functions
library(tidyverse)
library(readr)
require(ggplot2)
library(dplyr)

###Data Preparation
data_path = "../replication_data/finalDataCollection_out.csv" 

####Import data
d <- read_csv(data_path)
d_dim <- dim(d)
print(c("d dims: ", d_dim))
head(d)
```
```{r}
#### Data exclusion / filtering
# get the number of participants
num_workers_start <- length(unique(d$workerNum)) #before filtering
num_retake <- length(unique(d$workerNum[which(d$retake == 'Yes')])) #number who checked retake
num_vision_no <- length(unique(d$workerNum[which(d$vision == 'No')])) #number who failed vision check

print(c("started with ", num_workers_start, " participants"), quote = F)
print(c(num_retake, " indicated retake"), quote = F)
print(c(num_vision_no, " participants did not pass the vision test"), quote = F)

#get rid of workers who have taken the test before or don't meet the vision requirement and all grayscale and practice images
filtered_data <- d %>%
  filter(retake == 'No', vision == 'Yes', image_type != "grayscale", image_type != "practice")

#get the number of workers after filtering
num_workers_end <- length(unique(filtered_data$workerNum))
print(c("ended with ", num_workers_end, " participants"), quote = F)

# filter CSV for websites with too few ratings
website_count_filtered_data <- filtered_data %>%
  group_by(websiteName) %>%
  filter(n() >= 4) #exclude websites without at least 4 ratings

print(c("the new dims after filtering are: ", dim(website_count_filtered_data)))

num_before_websites <- length(unique(d$websiteName)) #number of websites at start
num_remove_grayscale_practice <- length(unique(filtered_data$websiteName)) #number of websites filtered for grayscale or practice
num_final_websites <- length(unique(website_count_filtered_data$websiteName)) #number of websites after filtering for count
print(c("started with ", num_before_websites, " website images"), quote = F)
print(c("got ", num_remove_grayscale_practice, " website images with grayscale and practice filter"), quote = F)
print(c("got ", num_final_websites, " website images after count filter"), quote = F)
```

```{r}
#demographic data

age_data <- filtered_data %>%
  group_by(workerNum) %>%
  summarize(agePerWorker = age[1]) %>%
  summarize(meanAge = mean(agePerWorker, na.rm = T), minAge = min(agePerWorker, na.rm = T), maxAge = max(agePerWorker, na.rm = T))
  
age_data
#look at gender of participants
print(c("mean age: ", age_data$meanAge), quote = F)
print(c("min age: ", age_data$minAge), quote = F)
print(c("max age: ", age_data$maxAge), quote = F)

gender_data <- filtered_data %>%
  group_by(workerNum) %>%
  distinct(workerNum, .keep_all = T) %>%
  mutate(isWoman = ifelse(gender == "Female", TRUE, FALSE), isMan = ifelse(gender == "Male", TRUE, FALSE), isOther = ifelse(!(gender == "Female" | gender == "Male"), TRUE, FALSE))

numWomen = sum(gender_data$isWoman)
numMen = sum(gender_data$isMan)
numOther = sum(gender_data$isOther)

#look at gender of participants
print(c("number of women: ", numWomen), quote = F)
print(c("number of men: ", numMen), quote = F)
print(c("number of other: ", numOther), quote = F)
```

### Methods Addendum (Post Data Collection)

#### Actual Sample
#Sample size, demographics, data exclusions based on rules spelled out in analysis plan
We started with 100 workers recruited on MTurk. `r num_vision_no` participants did not meet the vision criteria of normal or corrected-to-normal vision and color vision. `r num_retake` participants indicated they were retaking the test so their responses were excluded. Our final participant sample had `r num_workers_end` participants.

The original study authors provided 450 images. They excluded 15 images based on additional exclusion criteria for their subsequent models, so for our replication we started with these 435 website images from the original authors. In our analysis we excluded the practice and grayscale images as well as any images without at least 4 ratings. After this filtering stage, we were left with `r num_final_websites` website images to use in our analysis.


#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


##Results


### Data preparation
We collect data for both the computational image statistics as well as users' ratings of website image colorfulness. First, we begin with the authors' computed image statstics. Each of these different image measurements is treated as a single variable in our regression. For each user we collect their age, country of residence, native language, highest level of education, and if they have normal or corrected-to-normal vision. We exclude all data from participants without normal or corrected-to-normal vision.

For all of the analyses, the data are organized with each row is a different observation for user data (i.e., each participant, website, and trial matching). We merge the user data with the computational image metrics by adding columns to the data for each of the computational image metrics (16 HTML colors and 21 additional derived metrics).

Next, we determine which data to exclude from the analyses based on the original study's exclusion criteria. First, we compute which websites had three or fewer ratings and eliminate this data.

For the primary analysis: developing a model to compare the image metrics to users' ratings of colorfulness, we prepare our data for running multiple linear regression with backward elimination. To do this we start with all of our low-level image features as the independent variables and the participants' mean colorfulness scores as the dependent variable. We iteratively remove image feature variables if they do not make statistically significant contributions to the model and update the model.


```{r}
#### Prepare data for analysis - create columns etc.
#add a column for mean score
mean_score_data <- website_count_filtered_data %>%
  mutate(meanScore = (score + score2)/2.0) %>% #add the mean for each trial
  group_by(websiteName) %>% #group data by website name
  mutate(meanScore = mean(meanScore))


#all column names for model
myvars = c("websiteName", "meanScore", "black","silver","gray","white","maroon","red","purple","fuchsia","green","lime","olive","yellow","navy","blue","teal","aqua","hue","saturation","value","textArea","nonTextArea","numOfLeaves","percentageOfLeafArea","numOfTextGroup","numOfImageArea","colorfulness1","colorfulness2","colorHorizontalSymmetry","colorVerticalSymmetry","intensityHorizontalSymmetry","intensityVerticalSymmetry","intensityHorizontalBalance","intensityVerticalBalance","numOfQuadTreeLeaves_color","numOfQuadTreeLeaves_intensity","colorEquilibrium","intensityEquilibrium")
data_for_lm = mean_score_data[myvars]

data_for_lm_by_website <- data_for_lm %>%
  distinct(websiteName, .keep_all = TRUE)

data_for_lm_by_website<-na.omit(data_for_lm_by_website)

data_for_lm_by_website
dim(data_for_lm_by_website)
```

### Confirmatory analysis
The main contribution of the original paper's experiments that are the focus of this replication report is the model predicting users' ratings of a website's colorfulness from low-level image features of the website. We replicate this original model using our collected user colorfulness scores and the provided image statistics. We use multiple linear regression with backward elimination to eliminate not statistically significant image features and build our final model.

```{r}
#### Create model
#get initial fit
fit <- lm(meanScore ~ black+silver+gray+white+maroon+red+purple+fuchsia+green+lime+olive+yellow+navy+blue+teal+aqua+hue+saturation+value+textArea+nonTextArea+numOfLeaves+percentageOfLeafArea+numOfTextGroup+numOfImageArea+colorfulness1+colorfulness2+colorHorizontalSymmetry+colorVerticalSymmetry+intensityHorizontalSymmetry+intensityVerticalSymmetry+intensityHorizontalBalance+intensityVerticalBalance+numOfQuadTreeLeaves_color+numOfQuadTreeLeaves_intensity+colorEquilibrium+intensityEquilibrium, data=data_for_lm_by_website, na.action=na.exclude)

#do backward elimination
stepped_model <- step(fit, direction = "backward", trace=FALSE ) 

#show model summary
summary(stepped_model)
```

```{r}
#### plot data
pdf('model.pdf',width=6,height=6,paper='special')
ggplot(stepped_model$model, aes_string(x = fitted(stepped_model), y = data_for_lm_by_website$meanScore)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  scale_x_continuous(limits=c(1, 9), breaks=seq(1,9,1))+
  scale_y_continuous(limits=c(1, 9), breaks=seq(1,9,1))+
  labs(title = paste("Adj R2 = ",signif(summary(stepped_model)$adj.r.squared, 5),
                     "Intercept =",signif(stepped_model$coef[[1]],5 ),
                     " Slope =",signif(stepped_model$coef[[2]], 5),
                     " P =",signif(summary(stepped_model)$coef[2,4], 5)), x = "predicted colorfulness ratings", y = "observed colorfulness ratings") + theme_classic() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


###Exploratory analyses
After building our model, our goal is to compare the model from this replication to the model the original authors provide. To do this we compare the sets of statistically significant image features in the two models and compute the Jaccard index. 

Although the authors did not report feature correlations, we also examine the feature correlations to help build intuition about about why different image features were included in the final model. 

```{r}
#comparison with original model
significant_summary <- data.frame(summary(stepped_model)$coef[summary(stepped_model)$coef[,4] <= .05, 4])
sig_factors <- c(row.names(significant_summary)[2:dim(significant_summary)[1]])
original_sig_factors <- c("gray", "white", "maroon", "green", "lime", "blue", "teal", "saturation", "colorfulness2", "numOfImageArea", "numOfLeaves", "textArea", "nonTextArea")

in_both_models <- c(intersect(original_sig_factors, sig_factors))
only_in_original <- setdiff(original_sig_factors, in_both_models)
only_in_replication <- setdiff(sig_factors, in_both_models)
sig_models_union <- union(original_sig_factors, sig_factors)
jaccard_index <- length(in_both_models)/length(sig_models_union) #union divided by intersection

jaccard_index
in_both_models
only_in_original
only_in_replication


print(c("the image stats in both models are: ", in_both_models), quote = F)
print(c("the image stats in the original but not the replication are: ", only_in_original), quote = F)
print(c("the image stats in the replication but not the original are: ", only_in_replication), quote = F)
print(c("the jaccard index is: ", jaccard_index), quote = F)


#show model summary with feature correlations
summary(stepped_model, correlation = TRUE)
```

```{r}
corr_data <- summary(stepped_model, correlation = TRUE)$correlation
# corr_data2 <- data.frame(corr_data) %>%
#   filter_all(all_vars(abs(.) > .3))
# 
 
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

library(reshape2)
melted_cormat <- melt(reorder_cormat(corr_data))
head(melted_cormat)

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Correlation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1.1, 
    size = 6, hjust = 1), axis.text.y = element_text(vjust = 0, 
    size = 6, hjust = 1)) +
  coord_fixed() +
  labs(title = "Correlations between features", x = "Image features", y = "Image features")


diag(corr_data)=NA
#corr_data
#which(corr_data == max(corr_data), arr.ind = TRUE)
corr_data_table = data.frame(corr_data)

top_n(corr_data_table, 10)
o = order(corr_data_table, decreasing=TRUE)[1:10]
```

## Discussion

### Summary of Replication Attempt

In this replication, the goal is building a model of colorfulness similar to the original result and then comparing the two models. `r length(in_both_models)` out of `r length(original_sig_factors)` of the significant factors in the original model appeared in the replication model. "Green" and "Saturation" were the only features that were in the original model but not in the replication. The replication model, however, had `r length(only_in_replication)` additional significant features. The Jaccard index corresponding the comparison between the two models is `r round(jaccard_index, 2)`. This indicates the models have a moderate amount of overlap.

To follow up our comparison of the two models, we examined the correlations between different image features.

### Commentary
While we did not expect to exactly replicate the original model, we can gain some insight into the process by building a similar model. It seems like this type of model can easily be prone to overfitting, and there can be some variability in which image statistics are relevant. 

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.


```{r}
df2 = summary(stepped_model, correlation = TRUE)$correlation
hc = abs(df2) > 0.3
#hc = sort(hc)
#reduced_Data = df2[,-c(hc)]
#print (reduced_Data)
which(hc)
a <- which(abs(df2) > 0.5 & abs(df2) < 1.0,arr.ind=T)
rn <- data.frame(row.names(df2[a[,1],] )) #row names
cn <- data.frame(colnames(df2[,a[,2]]))     #column names

dim(rn)
dim(cn)

merge(rn, cn, by=0)
```

